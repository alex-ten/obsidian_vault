# How many trials do we need to fit parameters?

It depends on your goals. 

For example, if you were interested in the correlation between the fitted parameters and some personality trait, you can run simulations with known parameters to see if you can recover these from simulated data. 

## 1

Suppose you generate some personality trait data (you should have some idea of what is the average 5DCS curiosity score is, so generate a bunch of samples from that distribution. Now, using a known correlation coefficient $r^*$, generate specific model-parameter values. Assuming that 5DCS correlate with the LP weight $w_{LP}$, you can generate true model parameters $w^*_{LP}$. With these model parameters, you can now generate exploration data. You now have some data generated by a known (i.e., assumed) process with known parameters! If your parameter-fitting approach works, you should be able to recover both the parameter values and the correlation coefficient from the generated data. Let's say you fit LP weights to this generated data and get a bunch of estimates $\hat w_{LP}$. How well do these estimates correlate with 5DCS scores? If you are able to recover the original $r^*$, then your model has worked. You can repeat this procedure a many times arriving at different correlation coefficients and estimate the mean recovered correlation coefficient.

Now, to determine how many trials you need to fit the parameters, imagine recovering your models from 1, 2, or $N$ trials. You can plot the distribution of the recovered correlation coefficient across different values of $N$ to see at what $N$ you cat "close enough". Deciding what is "close enough" is up to you. This decision should be made on the basis of what the correlation coefficient that you are trying to estimate is supposed to be used for.

## 2

Now suppose you are interested in comparing different models. This is a pretty standard application of parameter recovery. First generate several bunches of parameters using different model variants and then generate data using these paramters. This is your ground truth. You know where each data was generated from. Now you can recover the parameters of each dataset assuming different models and see if the winning model[^1] for a given dataset corresponds to the true generating model. Again, you can repeat this procedure a bunch of times to get a sense of how reliable your parameter fitting procedure is. You can also construct a confusion matrix to see how many times one model is confused for some other model. A perfect result would be a matrix with diagonal terms equal to 1.0 and everything else 0.0. Here again, it is up to you to decide what level of confusion is good enough.


[^1]: To determine the winning model, you can use any model-comparison procedure that you use on your actual data.